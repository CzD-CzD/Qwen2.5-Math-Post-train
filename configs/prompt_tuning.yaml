output_dir: "out/prompt_math"
run_name: "prompt-math"
report_to: "swanlab"

# Prompt-tuning config
num_virtual_tokens: 20
# If you want explicit tokenizer, set this; otherwise omit or keep null
tokenizer_name_or_path:

# Train config (mirror full_sft.yaml)
max_length: 1024
num_train_epochs: 5
learning_rate: 5e-5
per_device_train_batch_size: 4
per_device_eval_batch_size: 4
gradient_accumulation_steps: 1

logging_steps: 10
eval_strategy: "steps"
eval_steps: 500

save_strategy: "epoch"

remove_unused_columns: false
bf16: true
fp16: false
dataset_text_field: "text"
